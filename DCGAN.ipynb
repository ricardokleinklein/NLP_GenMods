{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modelos Generativos\n",
    "\n",
    "## DCGAN - Imagen\n",
    "\n",
    "Creado por *Ricardo Kleinlein* para [Saturdays.AI](https://saturdays.ai/).\n",
    "\n",
    "Disponible bajo una licencia [Creative Commons](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sobre el uso de Jupyter Notebooks\n",
    "\n",
    "Este notebook ha sido implementado en Python, pero para su ejecución no es\n",
    "necesario conocer el lenguaje en profundidad. Solamente se debe ejecutar cada\n",
    "una de las celdas, teniendo en cuenta que hay que ejecutar una celda a la vez\n",
    "y secuencialmente, tal y como figuran en orden de aparición.\n",
    "\n",
    "Para ejecutar cada celda pulse en el botón ▶ en la esquina superior izquierda\n",
    "de cada celda. Mientras se esté ejecutando ese fragmento de código,\n",
    "el botón estará girando. En caso de querer detener dicha ejecución, pulse\n",
    "nuevamente sobre este botón mientras gira y la ejecución se detendrá. En caso\n",
    "de que la celda tenga alguna salida (texto, gráficos, etc) será mostrada\n",
    "justo después de esta y antes de mostrar la siguiente celda. El notebook\n",
    "estará guiado con todas las explicaciones necesarias, además irá acompañado\n",
    "por comentarios en el código para facilitar su lectura.\n",
    "\n",
    "En caso de tener alguna duda, anótela. Dedicaremos un tiempo a plantear y\n",
    "resolver la mayoría delas dudas que puedan aparecer.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objetivo del notebook\n",
    "\n",
    "Implementar, entrenar y evaluar una Red Generativa Adversarial (GAN) que\n",
    "genere números escritos \"a mano\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importamos las librerías necesarias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparar los datos de entrenamiento\n",
    "\n",
    "Los datos vienen en forma de imágenes en blanco y negro, de pocos píxeles de\n",
    " tamaño (28 x 28). Vamos a seleccionar un subset de 5000 de las 60000\n",
    " imágenes originales disponibles y a prepararlas para poder introducirlas\n",
    " como entrada a una GAN."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 32\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DCGAN\n",
    "\n",
    "De Deep Convolutional, la GAN que vamos a implementar es una arquitectura\n",
    "muy conocida, y esquemáticamente se basa en redes convolucionales con la\n",
    "siguiente arquitectura:\n",
    "\n",
    "![DCGAN](./assets/dcgan.png)\n",
    "\n",
    "No entraremos (al menos en principio, las dudas son bienvenidas) a detallar\n",
    "la estructura de la red, sino que simplemente la entrenaremos y usaremos\n",
    "para generar nuestras propias imágenes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Creamos el generador\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Creamos el discriminador\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sin entrenar\n",
    "\n",
    "¿Qué aspecto tendrá una imagen generada antes de entrenar?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x15ca7d320>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ¿Y qué opinará el discriminador?\n",
    "\n",
    "¿Pasará por buena?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.0002392]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Función de pérdidas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Función de pérdidas en particular para el discriminador\n",
    "\n",
    "Se encarga de minimizar los errores cometidos por la clasificación al\n",
    "predecir si una muestra es real o no."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Función de pérdidas en particular para el generador\n",
    "\n",
    "Se encarga de acercar lo máximo posible la semejanza entre las imágenes\n",
    "creadas por la red y las originales, penalizando las diferencias."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Elegimos un optimizador Adam (AVANZADO)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creación de checkpoints\n",
    "\n",
    "Como el modelo entrenará durante algunas iteraciones, nos interesa quedarnos\n",
    " únicamente con el modelo que mejor lo haga según nuestras funciones de\n",
    " pérdidas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrenamos!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in tqdm(range(epochs), total=epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez hemos definido el bucle de entrenamiento (es posiblemente una de las\n",
    " partes más difíciles de entender para alguien no experto en el tema),\n",
    " podemos entrenar propiamente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [03:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-8e2bb4a175d2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mEPOCHS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-b5304aa354d5>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(dataset, epochs)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mimage_batch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m       \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     generate_and_save_images(generator,\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    883\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    884\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 885\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    886\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    887\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    915\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 917\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    918\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3038\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   3039\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 3040\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   3041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3042\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1962\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1963\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1964\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1965\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1966\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    594\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    598\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#train(train_dataset, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Recogemos el mejor modelo\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x159d4ab38>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualización de resultados\n",
    "\n",
    "Vamos a crear un gif que nos permita ver de un único vistazo cómo ha ido\n",
    "evolucionando el proceso de aprendizaje de la GAN."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-4d99b7ab7501>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimageio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mwriter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m   \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimageio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m   \u001B[0mwriter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}