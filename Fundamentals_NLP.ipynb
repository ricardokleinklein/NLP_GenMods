{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)\n",
    "(https://colab.research.google.com/github/ricardokleinklein/NLP_GenMods/blob/main/Fundamentals_NLP.ipynb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Procesamiento de Lenguaje Natural (PLN)\n",
    "\n",
    "## Fundamentos de PLN\n",
    "\n",
    "Creado por *Ricardo Kleinlein* para [Saturdays.AI](https://saturdays.ai/).\n",
    "\n",
    "Disponible bajo una licencia [Creative Commons](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sobre el uso de Jupyter Notebooks\n",
    "\n",
    "Este notebook ha sido implementado en Python, pero para su ejecución no es\n",
    "necesario conocer el lenguaje en profundidad. Solamente se debe ejecutar cada\n",
    "una de las celdas, teniendo en cuenta que hay que ejecutar una celda a la vez\n",
    "y secuencialmente, tal y como figuran en orden de aparición.\n",
    "\n",
    "Para ejecutar cada celda pulse en el botón ▶ en la esquina superior izquierda\n",
    "de cada celda. Mientras se esté ejecutando ese fragmento de código,\n",
    "el botón estará girando. En caso de querer detener dicha ejecución, pulse\n",
    "nuevamente sobre este botón mientras gira y la ejecución se detendrá. En caso\n",
    "de que la celda tenga alguna salida (texto, gráficos, etc) será mostrada\n",
    "justo después de esta y antes de mostrar la siguiente celda. El notebook\n",
    "estará guiado con todas las explicaciones necesarias, además irá acompañado\n",
    "por comentarios en el código para facilitar su lectura.\n",
    "\n",
    "En caso de tener alguna duda, anótela. Dedicaremos un tiempo a plantear y\n",
    "resolver la mayoría delas dudas que puedan aparecer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo del notebook\n",
    "\n",
    "El objetivo de este notebook es mostrar las herramientas fundamentales\n",
    "usadas dentro del campo del NLP. En particular, aquellas centradas en la\n",
    "**limpieza y preparación de los datos** para usos posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué interesarnos en NLP?\n",
    "\n",
    "¿Por qué nos interesa tanto saber cómo analizar textos? En realidad, es una\n",
    "de las características que nos definen como especie. Si bien existe\n",
    "comunicación en otras especies animales, la nuestra es la única que\n",
    "conozcamos (¡al menos por el momento!) que es capaz de estructurar y\n",
    "compartir su pensamiento con otros individuos mediante una serie de reglas,\n",
    "que llamamos gramática, y una serie de abstracciones asociadas a ciertos\n",
    "sonidos llamadas palabras (Bottéro,2004).\n",
    "\n",
    "![escritura cuneiforme](./assets/cuneiform.jpg)\n",
    "\n",
    "Sin embargo, en cierto momento de la historia evolutiva de la Humanidad,\n",
    "nuestros antepasados pensaron que sería una buena idea desarrollar alguna\n",
    "manera de almacenar esos mensajes para que no se perdieran nada más ser\n",
    "expresados. Este proceso, que determina el comienzo de la Historia, dió luz\n",
    "a la escritura. Si bien al inicio sirvió mayoritariamente para llevar\n",
    "registros catastrales y financieros, no mucho después prácticamente la\n",
    "totalidad de las sociedades en el planeta habrían de adoptar uno u otro\n",
    "sistema de escritura elaborada (Cassin, 1971). Además, los registros dieron\n",
    "paso a épicas\n",
    "epopeyas, dramas románticos y muchos otros tipos de ficción.\n",
    "\n",
    "De alguna manera, mediante el lenguaje las personas tenemos acceso a\n",
    "información y conocimiento sobre la forma de pensar, actuar, las intenciones\n",
    "y deseos de otra(s) persona(s). En particular, el lenguaje escrito ha\n",
    "cobrado especial relevancia en las últimas décadas (no digamos ya en la\n",
    "última) por el auge de las tecnologías digitales.\n",
    "\n",
    "¿Quién no ha mirado ha leído un e-mail hoy?\n",
    "\n",
    "¿Cómo se llamaba la última canción que escuchaste?\n",
    "\n",
    "¿Cuáles son los titulares del periódico de hoy?\n",
    "\n",
    "¿Qué están diciendo mis clientes de mi último producto en las redes sociales?\n",
    "\n",
    "...\n",
    "\n",
    "La lista de preguntas cuya respuesta se basa de una manera u otra en\n",
    "interpretar y entender lo que otros dicen es infinita. No sorprende que las\n",
    "empresas de todo el mundo quieran hacer un mejor uso de esta información\n",
    "para ser más eficientes y desarrollar su actividad de manera más eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos las librerías necesarias\n",
    "\n",
    "Para no tener que preocuparnos más tarde, vamos a cargar en este momento\n",
    "todas las librerías necesarias.\n",
    "\n",
    "Recuerde, **una librería es tan sólo un conjunto de herramientas ya\n",
    "programadas** de tal forma que podemos centrarnos en otros aspectos del\n",
    "trabajo sin tener que escribir todo el código de cero cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mgutenberg\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/gutenberg\u001B[0m\n\n  Searched in:\n    - '/Users/ricardokleinlein/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/share/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m                     \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.subdir}/{zip_name}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/data.py\u001B[0m in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 583\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    584\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mgutenberg\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/gutenberg.zip/gutenberg/\u001B[0m\n\n  Searched in:\n    - '/Users/ricardokleinlein/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/share/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-47-794cec52a92f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mstem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbook\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/book.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Type: 'texts()' or 'sents()' to list the materials.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m \u001B[0mtext1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mText\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgutenberg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"melville-moby_dick.txt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"text1:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, attr)\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 121\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    122\u001B[0m         \u001B[0;31m# This looks circular, but its not, since __load() changes our\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;31m# __class__ to something new:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     84\u001B[0m                     \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.subdir}/{zip_name}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 86\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m         \u001B[0;31m# Load the corpus.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 81\u001B[0;31m                 \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.subdir}/{self.__name}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     82\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/Personal/saturdays_ai/PYTHON/lib/python3.6/site-packages/nltk/data.py\u001B[0m in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    581\u001B[0m     \u001B[0msep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"*\"\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m70\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 583\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    584\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mgutenberg\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/gutenberg\u001B[0m\n\n  Searched in:\n    - '/Users/ricardokleinlein/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/share/nltk_data'\n    - '/Users/ricardokleinlein/Desktop/Personal/saturdays_ai/PYTHON/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string   # Operaciones sobre texto\n",
    "import nltk     # Natural Language Toolkit\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk import corpus\n",
    "from nltk import stem\n",
    "from nltk import book\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "NLTK no es la única librería de NLP que existe. [spaCy](https://spacy.io/),\n",
    "[TextBlob](https://textblob.readthedocs.io/en/dev/),\n",
    "[Gensim](https://radimrehurek.com/gensim/) o\n",
    "[CoreNLP](https://stanfordnlp.github.io/CoreNLP/) son otras librerías\n",
    "comúnmente usadas. Sin embargo, NLTK se ha impuesto como el estándar \"de\n",
    "facto\". Es por ello que vamos a centrarnos en ella, si bien no es raro\n",
    "mezclar herramientas en una misma aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ricardokleinlein/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Puede tardar unos minutos\n",
    "\n",
    "nltk.download([\"names\", \"stopwords\", \"state_union\", \"twitter_samples\",\n",
    "              \"movie_reviews\", \"averaged_perceptron_tagger\",\n",
    "              \"vader_lexicon\", \"punkt\", \"wordnet\", \"book\", \"gutenberg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenización de palabras o frases\n",
    "\n",
    "Un sistema computacional no entiende de manera natural qué es una palabra y\n",
    "qué no. A efectos reales, todo lo que el ordenador interpreta es una\n",
    "secuencia de caracteres alfanuméricos (los espacios en blanco no dejan de\n",
    "serlo).\n",
    "\n",
    "Así, en NLP se trabaja con una unidad mínima de significado denominada\n",
    "grama (gram en inglés). Se dice que un unigrama (**1-gram**) equivale a una\n",
    "palabra, un bigrama (**2-gram**) a 2 palabras juntas, como San Francisco,\n",
    "Nueva York o Papa Francisco, y de manera similar para órdenes mayores\n",
    "(3-gram, 4-gram...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera frase:\n",
      "the crew of the uss discovery discovered many discoveries, some of them still undiscovered.\n",
      "Primera palabra:\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "example = \"The crew of the USS Discovery discovered many discoveries, some \" \\\n",
    "          \"of them still undiscovered. Discovering is what explorers do.\"\n",
    "\n",
    "# De hecho es habitual trabajar con todo minúsculas:\n",
    "example = example.lower()\n",
    "\n",
    "sentences = tokenize.sent_tokenize(example)    # Tokeniza a nivel frase\n",
    "print(f'Primera frase:\\n{sentences[0]}')\n",
    "words = tokenize.word_tokenize(example) # Tokeniza a nivel de palabra\n",
    "print(f'Primera palabra:\\n{words[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pero fijémonos en qué ocurre cuando le pedimos mostrar todas las palabras\n",
    "detectadas por este proceso de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'crew', 'of', 'the', 'uss', 'discovery', 'discovered', 'many', 'discoveries', ',', 'some', 'of', 'them', 'still', 'undiscovered', '.', 'discovering', 'is', 'what', 'explorers', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observamos que si bien reconoce correctamente todas las palabras, la coma\n",
    "presente en la primera frase (y lo mismo ocurre con puntos, apóstrofes y\n",
    "otros signos de puntuación) también ha sido incorrectamente reconocida como\n",
    "una palabra. Este es un problema común, para el cual la solución es simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'crew', 'of', 'the', 'uss', 'discovery', 'discovered', 'many', 'discoveries', 'some', 'of', 'them', 'still', 'undiscovered', 'discovering', 'is', 'what', 'explorers', 'do']\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(sentence):\n",
    "  return sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "clean_example = remove_punctuation(example)\n",
    "words = tokenize.word_tokenize(clean_example)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mediante esta función, podemos eliminar los signos de puntuación más comunes\n",
    "que se encuentran en textos escritos. Dependiendo del idioma y de ciertos\n",
    "detalles técnicos, se pueden requerir métodos más complejos de filtrado de\n",
    "caracteres (regex).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Eliminación de palabras comunes\n",
    "\n",
    "No todas las palabras de una oración tienen la misma relevancia para\n",
    "nosotros a la hora de analizar un texto. Piense en determinantes,\n",
    "proposiciones... Si bien contribuyen a dotar al texto de significado, no\n",
    "soportan el peso de la narración.\n",
    "\n",
    "A estas palabras se les denomina en inglés *stopwords*, y podemos\n",
    "eliminarlas fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de stopwords en inglés:\n",
      "['through', 'mustn', 'an', 'o', 'll', 'some', 'was', 'are', 'should', 'that', 'up', 'just', 'hadn', 'to', 'won', 'is', 'this', 'other', 'd', \"doesn't\", 'so', 'themselves', 'isn', \"you've\", 'once', 'the', 'under', 'all', \"won't\", 't', 'yourselves', 'your', 'into', 's', 'same', 'hasn', 'mightn', \"should've\", 'while', 'having', 'me', 'off', 'such', \"wasn't\", 'do', \"isn't\", 'have', \"she's\", 'y', 'does', 'ma', 'below', 'but', \"wouldn't\", \"hadn't\", 'after', 'from', 'no', 'out', 'above', 'needn', 'doing', 'at', 'between', 'wouldn', 'against', \"mustn't\", 'he', 'being', 'nor', 'further', 'more', \"you'd\", 'what', 'yourself', 'any', \"hasn't\", 'shan', 'its', 'yours', 'if', 'weren', 'by', 'don', 'only', 'will', 'before', 'of', 'there', 'those', 'each', 'herself', 'where', 'm', \"that'll\", 'during', \"it's\", 'wasn', 'for', 'haven', 'couldn', \"couldn't\", 'shouldn', 'here', 'ourselves', 'were', \"you'll\", 'they', 'we', 'it', 'been', 'and', 'as', \"aren't\", \"needn't\", 'down', 'her', 'a', \"mightn't\", \"weren't\", 'them', 'myself', 'itself', 'hers', \"shouldn't\", 'who', 'over', 'own', 'she', 'now', 'ain', 'which', \"you're\", \"haven't\", 'than', 'has', 'had', 'our', 'because', 'on', 'ours', 'why', 'these', 're', 'am', 'again', 'you', 'whom', 'too', 'theirs', 'most', 'himself', 'then', 'with', 'until', 'my', \"don't\", 'very', 'about', 'their', 'him', 'how', 'when', 'aren', \"didn't\", 'can', 'be', 'doesn', 'in', 've', 'both', 'i', 'his', 'not', 'did', \"shan't\", 'didn', 'or', 'few']\n",
      "\n",
      "Frases sin puntuación ni stopwords:\n",
      "crew uss discovery discovered many discoveries still undiscovered discovering explorers\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(set(corpus.stopwords.words('english')))\n",
    "print(f'Lista de stopwords en inglés:\\n{stop_words}\\n')\n",
    "\n",
    "words = [word for word in words if word not in stop_words]\n",
    "print('Frases sin puntuación ni stopwords:\\n' + ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Como podemos observar, si bien la diferencia que percibimos con respecto al\n",
    "texto original es enorme, uno puede convencerse de que la información\n",
    "esencial permanece. Tenemos nombres propios, objetos, lugares... Podríamos\n",
    "decir que conserva en esencia el mensaje, con un número menor de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras en el original: 19\n",
      "Sin stopwords: 10\n"
     ]
    }
   ],
   "source": [
    "nb_original = len(tokenize.word_tokenize(clean_example))\n",
    "print(f'Número de palabras en el original: {nb_original}')\n",
    "print(f'Sin stopwords: {len(words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "¡Menos de la mitad de las palabras!\n",
    "\n",
    "Aquí hemos utilizado un conjunto de palabras \"default\" proporcionadas dentro\n",
    "de la librería NLTK, pero éste puede modificarse a voluntad, tanto como\n",
    "para extender dicha lista como para acortarla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracción de la raíz de palabras\n",
    "\n",
    "Existen dos procedimientos muy comúnmente usados:\n",
    "\n",
    "### Stemming\n",
    "\n",
    "Las palabras, al menos en el lenguaje actual, no están limitadas a una única\n",
    "forma exclusiva. Por ejemplo, el concepto de \"ayuda\", da pie a muchas otras\n",
    "similares: \"ayudante\", \"ayudas\", \"ayudo\", \"ayudamos\", y un largo etcétera.\n",
    "\n",
    "En muchas tareas nos interesará trabajar únicamente con la forma más corta\n",
    "que, de alguna manera, encapsule el concepto fundamental contenido en todas\n",
    "las anteriores palabras. Dentro de la librería NLTK hay varias herramientas\n",
    "que sirven para realizar este proceso, pero nosotros vamos a ver el caso\n",
    "particular del algoritmo de Porter, basado en eliminar de manera iterativa\n",
    "sufijos dentro de una palabra (Porter, M.F. (1980))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crew uss discoveri discov mani discoveri still undiscov discov explor\n"
     ]
    }
   ],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "stem_example = [stemmer.stem(word) for word in words]\n",
    "print(' '.join(stem_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El resultado dista de ser perfecto.\n",
    "\n",
    "- Understemming: Ocurre cuando dos palabras relacionadas deberían ser\n",
    "reducidas a una misma raíz, pero no lo son.\n",
    "- Overstemming: Al contrario, cuando dos palabras no relacionadas se reducen\n",
    " a una misma raíz incorrectamente.\n",
    "\n",
    "Este es sólo un ejemplo, y la propia librería NLTK tiene procesadores de\n",
    "Stemming más recientes, o en otros idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobahn\n"
     ]
    }
   ],
   "source": [
    "german_stemmer = stem.SnowballStemmer('german')\n",
    "print(german_stemmer.stem('Autobahnen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "De manera análoga al stema, el procedimiento de lemmatization permite\n",
    "reducir la complejidad del vocabulario de un texto. La diferencia es que el\n",
    "lemma consiste en una palabra con significado propio (el **lexema**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crew us discovery discovered many discovery still undiscovered discovering explorer\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = stem.WordNetLemmatizer()\n",
    "lemma_example = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(' '.join(lemma_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Concordancia\n",
    "\n",
    "El concepto de concordancia nos permite ver las veces que una palabra es\n",
    "usada, junto con su contexto inmediato. Si bien es cierto que este\n",
    "procedimiento no es estrictamente un proceso de filtrado o de limpieza de\n",
    "datos, entra dentro del rango de tareas a realizar de modo preliminar para\n",
    "comprender cómo son los datos que trabajamos.\n",
    "\n",
    "Para el resto del notebook, vamos a trabajar con un texto de muestra más\n",
    "largo, contenido dentro de NLTK. En particular, vamos a trabajar con la\n",
    "versión original en inglés de Moby Dick.\n",
    "\n",
    "Veamos en qué contextos aparece la palabra marinero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "moby_dick = book.text1\n",
    "moby_dick.concordance('sailor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Collocation\n",
    "\n",
    "Muy relacionado se encuentra la idea de \"collocations\", o conjuntos de\n",
    "palabras que aparecen con frecuencia juntas (bigramas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "moby_dick.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Como era de esperar, las palabras relativas a los protagonistas principales,\n",
    " así como a la ballena, conforman binomios de palabras que aparecen si no\n",
    " siempre, casi siempre juntas.\n",
    "\n",
    "Asímismo, observamos bigramas repetidos donde la diferencia consiste en el\n",
    "uso de mayúsculas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "Bottéro, J. (2004) Mesopotamia: La Escritura, La Razón y Los Dioses. Cátedra,\n",
    "Grupo Anaya S.A. ISBN: 84-376-2119-4.\n",
    "\n",
    "Cassin, E., Bottéro, J. and Vercoutter, J. (1971). Los imperios del Antiguo\n",
    "Oriente. I. Del paleolítico a la mitad del segundo milenio. Siglo Veintiuno\n",
    "ISBN: 978-84-323-0039-4\n",
    "\n",
    "Porter, M.F. (1980), \"An algorithm for suffix stripping\", Program:\n",
    "electronic library and information systems, Vol. 14 No. 3, pp. 130-137.\n",
    "https://doi.org/10.1108/eb046814"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}